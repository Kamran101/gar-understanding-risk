## Exposure <sup>17</sup>
Exposure modelling has a critical role to play in risk assessment. Empirical studies suggest that the greatest influence on output loss estimates from risk models derives from exposure data, as opposed to either hazard or vulnerability data (see for example Bal et al. 2010; Chen et al. 2004; Spence et al. 2003; Lavakare and Mawk 2008). 

The process of exposure modelling identifies the elements at risk in areas that could potentially be affected by natural hazard events (UNISDR 2009; Ehrlich and Tenerelli 2013; van Westen 2012). In other words, if a hazard occurs in an area with no exposure, there is no risk. This is the case, for example, with an earthquake in an unpopulated area of Alaska. 

Exposure modelling techniques have been developed at various scales, from global to local. Significantly, global-scale and local-scale modelling use different methodologies: the former tends to take a top-down approach, with work being carried out by governments or large institutions, whereas the latter works from the bottom up by methods such as crowdsourcing and in situ surveys. At least four homogeneous inventory regions—urban residential, urban nonresidential, rural residential, and rural nonresidential—are usually defined to capture the differences in occupancy and construction. Data sources also vary by resolution.

At the **local scale**, high-resolution exposure data have been developed on an ad hoc basis, in areas where risk modelling has been carried out. Crowdsourcing has become a common and valuable tool for collecting detailed bottom-up data, but this approach has limits, both in the type of data it can collect and in the quality of those data. In addition to being used to develop exposure data at a local scale, crowdsourcing has also been used to validate global-scale data. At the **national scale**, complete geospatially linked inventories that include public infrastructures are rare and not publicly available in most developing countries, where exposure model development is most needed for risk assessments. At the **global scale**, efforts to generate globally consistent exposure data sets in terms of quality and resolution have increased. Experience has shown that development of exposure data sets requires innovative, efficient methodologies for describing, collecting, validating, and communicating data, while also accounting for the inherent spatiotemporal dynamics associated with exposure—that is, the dynamics by which exposure evolves over time as a result of (unplanned) urbanization, demographic changes, modifications in building practices, and other factors. 

The information used to develop exposure data sets can be derived from various sources and methods. At a local level, common data sources are council and local government agencies, household surveys, aerial photos, and individual architectural/structural drawings. At a regional level and above, state-based agencies, statistical offices, census data, investment and business listings, employment figures, and existing geographic information system (GIS) data are common sources of exposure information. At the coarsest level of resolution, national statistical agencies, census data, global databases, and remote sensing are used for developing exposure data. 

Commercial risk models have developed the so-called industry exposure databases for regions where risk models are offered. These exposure data can include detailed information on construction as well as estimates of the value of the contents within a structure. The resolution of the exposure data is typically at the postal code level with varying levels of occupancy types. However, these data are almost always proprietary. 

The classification (taxonomy and ontology) used to generate these exposure data varies from data set to data set; this variation is problematic for efforts to merge independently developed data sets. Nor is there a commonly agreed upon taxonomy that accounts for features such as construction attributes and asset valuation across different hazards. 

In recent years, several data sets with global coverage have made the first step in overcoming these obstacles. The first such global exposure data set was developed in 2010 for PAGER (Prompt Assessment of Global Earthquakes for Response), a global near-real-time earthquake loss estimation system, by the U.S. Geological Survey (Jaiswal, Wald, and Porter 2010a). In addition, three global exposure databases are slated for publication in 2014, the global risk model by UNISDR (De Bono 2013; see section 3-7), the GED4GEM by the Global Earthquake Model (Dell’Acqua, Gamba, and Jaiswal 2012; see section 3-6), and the World Bank exposure database, which will be completely open and suitable for multi-hazard analyses (Gunasekera et al. 2014).<sup>18</sup> Many of these newer exposure models take advantage of aspects of building typology taxonomy originally compiled in the PAGER database. Several examples of global exposure data sets are given in box 2-5.

///Categories of information included in exposure models./// There are several categories of assets that need to be included in a comprehensive exposure model (table 2-2). The broad variety of categories illustrates the necessity of combining efforts from different disciplines, such as geographical science, statistics, engineering, mathematics, economics, remote sensing, and socio-demographics.

It is clear that as more data are integrated, modelled, and jointly analyzed, __uncertainties__ propagate in the model and in the subsequent results. A choice needs to be made about whether slightly more-detailed data will improve a model or merely add to the noise and confusion. The impossibility of eliminating uncertainty in hazard and vulnerability modelling is widely recognized. After all, every model constitutes a simplified approximation of reality. Depending on geospatial data characteristics (including resolution aspects) and integration factors, uncertainty may increase. It is therefore essential for uncertainties to be conceptually integrated into the framework of the risk analysis, and consequently into the loss estimates. The uncertainties and associated limitations in the final risk assessment then need to be communicated to the end-users of this information.

///Information required for the modelling of physical damage./// On a national scale, reliable data on physical exposure are less available than population data. Information is often missing or incomplete, and few governments have developed national exposure databases of buildings and infrastructure that are open and can be used to understand the impacts of multiple hazards (Turkey, Australia, the United States, and New Zealand are exceptions). Thus it is not surprising that most exposure data sets at the national scale or above use the spatial distribution of population as a proxy for developing exposure estimates. This is a rapidly evolving area, however, and more governments are seeing the widespread value of developing exposure information.

The basic information needed to model the response of a structure to a hazard event includes its location, occupancy, construction type, length or density (for road and railway), and replacement value. The response of a structure to a hazard event can be more realistically simulated using additional structural information such as its square footage, shape, height, age, roof type, irregularities, and material and mechanical properties, as well as building codes applicable to it. For hydrological hazards, additional details useful for vulnerability assessments include information on the height above ground of the first occupied floor, distance from water channels, and the presence of basements. Knowledge of the replacement value makes it possible to estimate the direct loss associated with an event. 

///Modelling economic losses./// Valuation data are critical for quantitatively assessing economic loss from disasters. The reinsurance industry uses claims and other economic data sets to calibrate its exposure models. However, this information is often proprietary and limited to insured risks. Obtaining comprehensive loss data for uninsured property is much more difficult. Proxy data such as socioeconomic surveys, labor statistics by economic sector, floor area per employee by type of activity, etc. are used to determine nonresidential building stock values. Accounting for a structure’s contents becomes particularly significant when modelling nonresidential occupancy classes. 

///Incorporating the temporal variation in human exposure./// Other important factors related to exposure data are population and demography characteristics that highlight the movement of population through the course of a day. Consider, for example, the swelling of populations in major metropolitan areas during the work day, or the varying population characteristics of areas of cultural or religious value depending on the day and/or time of the year. Temporal variability in human exposure can be a key factor in determining the impact of rapid onset events such as earthquakes, landslides, or tsunamis. Models of building occupancy that consider daily patterns have been proposed (Coburn and Spence 2002; Coburn, Spence, and Pomonis 1992), but collecting the necessary data to update such models can be very time- and resource-intensive. A promising alternative approach takes advantage of cellular phone data provided by telephone companies (Wesolowski et al. 2013; Lu et al. 2013).

///Exposure data collection approaches—full enumeration, sampling, or disaggregation using proxy data./// In general terms, top-down and bottom-up approaches are used to collect exposure data. Approaches that use bottom-up methods commonly employ direct observation, which relies on two principle strategies: full enumeration or sampling. With the __full enumeration__ approach, each exposed asset in the study area is detected and defined. This approach can be very accurate and detailed but also requires a greater expenditure of time and other resources. Census data are commonly used to fully enumerate human populations, though this approach is best suited to developed countries, which are likely to have slow or moderate population growth and up-to-date census data. __Volunteered geographic information__ (VGI), another approach to full enumeration, derives data from the joint efforts of many individuals who voluntarily collect and submit data. VGI may be either structured or unstructured—the latter applies to unsystematic, non-authoritative initiatives such as OpenStreetMap (see box 1-2), which rely on participants’ interest and motivation. The structured approach also involves volunteers but has an authoritative component that directs volunteers’ efforts toward certain tasks (Chapman 2012), such as a government-led participatory mapping program to collect exposure data for risk assessment (see section 3-3). 

With a __sampling__ approach, summary statistics for a large area are estimated based on smaller subset areas. Increasingly, census methodologies are turning to sampling and statistical modelling rather than full enumeration because they provide more up-to-date and more accurate information with less effort than traditional methods. A rolling census approach—in which only small areas are fully enumerated and other, highly populous areas are continuously sampled at the rate of around 10 percent a year—makes it possible to update data annually instead of every 5 to 10 years (UN Statistical Division 2008). 

__Remote sensing__ is on occasion used in conjunction with these sampling methodologies (Adams and Huyck 2006; Müller, Reiter, and Weiland 2011; Geiß and Taubenböck 2013). For instance, urban areas can be classified according to their density using satellite images, followed by a sampling approach where high-resolution imagery (manual or automatic extraction of features) or direct observation is used to fully enumerate assets (buildings, roads, bridges) and their geometric characterization (footprint, shape, height) within each of the sampling areas that represent the common density pattern classified during the first step. Alternatively, if time and resources permit, optical satellite or aerial images can be used to extract all of the footprints for buildings in an exhaustive manner. To provide a complete description of the exposure, however, the footprints should be combined with in situ direct observations or other data sets (such as national statistics information) that provide additional data that cannot be captured from above (e.g., construction features or building use).

In recent years, digital __in situ data capturing systems__ have started to emerge, which allow the user to collect and generate exposure information using handheld direct observation tools in combination with other disaggregation or extrapolation methodologies (FEMA 2002). An example includes the open source suite of tools developed under GEM called Inventory Data Capture Tools (IDCT). IDCT takes information generated from the analysis of satellite images to characterize built-up areas and combines it with sampled direct field observations on individual buildings using handheld devices or paper survey forms. This information is then integrated through the use of mapping schemes to generate exposure information. 

Indirect, top-down disaggregation approaches use exposure proxies to develop exposure data sets when direct observation alone is not feasible. Information on the spatial distribution of population and built-up areas allows the exposure to be disaggregated into finer resolutions. Some examples of this approach are described in box 2-6.

///Multi-source integration./// The growing variety of possible exposure information sources requires the flexible integration of existing information from different acquisition techniques, scales, and accuracies, so that no available information is discarded. An example for a probabilistic integration approach is given in Pittore and Wieland (2013). This method is based on Bayesian networks and allows for the sound treatment of uncertainties and for the seamless merging of different data sources, including legacy data, expert judgment, and inferences based on data mining.

There are clearly many approaches to collecting exposure data; however, for best results the decision on the approach must be aligned with the scale and purpose of the risk assessment (see box 2-7).
