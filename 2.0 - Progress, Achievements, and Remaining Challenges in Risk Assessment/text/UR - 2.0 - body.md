Risk assessments require hazard, exposure, and vulnerability data at the appropriate scale as well as models with the appropriate resolution to address the problem of interest. They also require a considered approach to building multidisciplinary, multi-institutional platforms and nontraditional partnerships around the technical analysis. In this section, we discuss these aspects by reviewing promising innovations in risk assessment over the last decade and highlighting some of the greatest remaining challenges.

### Hazard Assessment
Essential steps required to quantify risk are the identification of the relevant hazard(s) and the collection of hazard-related data. Although these steps usually occur at the start of a risk assessment, they are often not easy or straightforward. They often involve deciding whether to undertake a single hazard or multi-hazard assessment of the primary hazards and then deciding whether to consider secondary (or cascading) hazards that may be triggered by a primary hazard event—for example, fire or tsunami after earthquake.  
 
These are not simple decisions. Since it is a rare country or community that is affected by only a single hazard, assessments that consider the full range of hazard events often achieve greater traction; on the other hand, the level of investment for considering all hazards may be too great, or momentum following a disaster event may be driving interest in single hazard. Adding the complexity of secondary hazards will further increase the resource and data requirements and may significantly broaden the institutions involved in a risk assessment. For example, considerations of fire after an earthquake require additional data sets, as well as engagement with fire authorities and energy and water companies. These challenges are discussed further in box 2-1. 

Once the hazards of interest are defined, the next step often involves acquiring a variety of hazard-related data. The most fundamental data define historical events, in particular their date, geographical location and extent, and maximum intensity. Historical events are often used in deterministic analyses that assess the impact of past events with current exposure. Historical event information is also used to estimate the probability of a hazard occurring at a location with a specific intensity. 

An __event set__ comprises a suite of stochastic, or computationally generated, synthetic hazard events with statistical characteristics consistent with the historical record. Such event sets can typically include thousands or tens of thousands of potential events and are intended to define the full range of potential events for a hazard. Event sets are used with other information to quantify probabilities of loss and risk from a hazard. 

Additional information is used to define the spatial distribution of the forces (e.g., the wind field from a tropical cyclone or the ground motion from an earthquake) associated with a hazard event. Such information is often incomplete or unavailable and in most cases must be derived from a very limited set of observations. Typically, a combination of observational data and theory is used to define the spatial and temporal characteristics of an event. A collection of the spatial, intensity, and temporal characteristics for events in an event set is termed a __hazard catalog__.
 
Hazard catalogs and event sets can be used with risk models in a deterministic or probabilistic manner. Deterministic risk models are used to assess the impact of specific events on exposure. Typical scenarios for a deterministic analysis include renditions of past historical events, worst-case scenarios, or possible events at different return periods.<sup>12</sup> For example, a deterministic risk (or impact) analysis will provide a robust estimation of the potential building damage, mortality/morbidity, and economic loss from a single hazard scenario. Risk models are used in a probabilistic sense when an event set contains a sufficient number of events for the estimate of the risk to converge at the longest return period, or the smallest probability, of interest. In other words, a probabilistic risk model contains a compilation of all possible “impact scenarios” for a specific hazard and geographical area. Note that hazard catalogs are generally associated with rapid onset hazards. Risk assessments for slow onset hazards, such as drought, are typically undertaken using deterministic approaches. (Additional issues associated with modelling drought risk and impacts are discussed in box 2-2. For a cost-benefit approach to risk that deals with the effects of drought on livestock, see box 2-3).

Convergence of results is a concern when using a risk model probabilistically. As a simple example, consider a simulation of 100 years of hazard events. This simulation is too short to determine the 100-year return period. A random sample of 100 years of events could easily omit events, or include multiple instances of the same event, that on average would occur once every 100 years and therefore dramatically affect determination of return period.

Figure 2-1 illustrates this challenge. If the sample size (1900 and after) is the historical record, then it would appear that extreme flood and drought are not a concern. Similarly, if we consider the period 1800–1900, flood would be seen as a risk, but not drought. Herein lies the challenge of determining the return period for rare and extreme hazard events. In the case of hydrometeorological cycles, determining the return period is difficult; for geophysical hazards such as volcanic eruptions and large earthquakes, which may occur every 1,000, 10,000, or 100,000 years, it is incredibly complex.

A variety of hazard-dependent data are required to generate a hazard catalog. Knowledge of the distribution of soil types, for example, is required to model the spatial variation of ground acceleration (shaking) from an earthquake; values for surface roughness are needed to define the distribution of wind speed from a tropical cyclone; and a digital elevation model (DEM) is needed to determine flood depth. Fortunately, some data can be common to multiple perils. For example, topography as defined by a DEM is required for modelling floods, tsunamis, sea-level-rise inundation, landslide susceptibility, storm surges, and detection of earthquake fault lines. 

Hazard data can be open, proprietary, or (if they have yet to be collected) unavailable. Even available data may be usable to different degrees—for example, data may not be digitized, may lack necessary metadata, or may require substantial improvement before use. A compilation of publicly available hazard-related data with global coverage is given in table 2-1. Some of these data sets, such as the records for the location and intensity of earthquakes and tropical cyclones, provide global coverage and are considered authoritative records that compile the best available data.<sup>13</sup> Other global data sets may not be of optimal quality for risk assessment. For example, openly available topographic data are not optimal for modelling hydrometeorological hazards because of their relatively coarse resolution. Poor resolution of elevation data has a significant impact on flood risk, since small changes in elevation can involve huge changes in the predicted inundation area in many relatively flat floodplains and coastlines. 

The spatial characteristics of an event are usually defined by combining theoretical and empirical knowledge with other observational hazard-related data because of the sparseness of the relevant observations. For example, quantifying the wind field for a tropical cyclone as it travels inland highlights the difficulty of estimating the spatial distribution of a hazard. Wind speed and pressure measurements from observing stations can be used to estimate two parameters, a cyclone’s maximum wind and the radius of maximum wind. It is often impossible to obtain high-quality measurements, however: the number of observational platforms is limited, existing observation stations are not sited optimally, power may fail during the cyclone, and anemometers may be damaged by flying debris. Surface pressure measurements of the cyclone are easier to collect, and the minimum central pressure has a large influence on maximum wind speeds, but these surface pressures must be converted to surface wind speeds for risk modelling purposes, and this is where the theoretical and empirical knowledge is critical. 

Most hazard event sets and catalogs are developed region by region. Exceptions include the global earthquake event set generated by the Global Earthquake Model (GEM), and the tsunami, volcanic eruption, cyclone, and drought hazard event sets developed as part of the global risk model under the leadership of the UN Office for Disaster Risk Reduction. There are also a number of efforts to develop global flood models, which will use a global flood catalog; one model, GLOFRIS (GLObal Flood Risk with IMAGE Scenarios), is already in use (see section 3-23 for a more detailed discussion).

A critical requirement acknowledged by all experts working in hazard modelling is the need for a high-resolution, open DEM. Currently, the 90m Shuttle Radar Topography Mission (SRTM) is the only global open DEM, with 30m resolution available in some countries. Satellite-based Interferometric Synthetic Aperture Radar (InSAR) appears to be one promising approach for generating these data on a global scale; one satellite currently using InSAR is the TerraSAR/Tandem-X of DLR (German Aerospace Center) and Astrium Geo-Information Services. A growing alternative to a satellite-based collection of elevation data is the use of airplanes and/or helicopters to derive high-resolution surface data on a smaller scale via LiDAR<sup>14</sup> or airborne InSAR. Both of these “active” methods, while expensive, are capable of generating very accurate and high-resolution surface and terrain elevations. Collection of LiDAR data is growing across the globe; however, the cost, time, and technical processing aspects of this approach prohibit its widespread accessibility. 

There are two types of DEMs: a digital surface elevation model and a digital terrain model. A digital surface elevation model provides surface elevations that describe the elevations of features such as buildings and treetops. A digital terrain model provides elevations of the bare ground surface and neglects objects such as buildings and trees. The impact of the different models on hazard and risk assessments can be significant—see box 2-4—but the combination of these different DEMs offers opportunities for better characterizing the built environment.<sup>15</sup>

To assess risk from multiple meteorological hazards on a global scale, one should consider the hazards’ spatial and temporal correlations and how they vary as a function of climate. For example, the probability of tropical cyclone landfall varies as a function of the El Niño-Southern Oscillation (ENSO) along the Queensland coast of Australia, the U.S. coastline, and in the northwest Pacific. Generally, warm El Niño years are associated with a reduced rate of landfall, and cool La Niña years are associated with a higher rate of landfall (Flay and Nott 2007; Elsner and Jagger 2006; Wu, Chang, and Leung 2004). There are also possible cross-peril correlations. In many areas, for example, flood risk and drought risk are strongly correlated with ENSO.

The response of meteorological hazards to natural climate variability highlights the possibility that the risk from these hazards will respond to future changes in climate. It is difficult to specify with certainty how hazard occurrence and intensity will change by region, and this is an area of significant research and modelling.<sup>16</sup> S A case study described in section 3-24 highlights the changing risk associated with future changes in tropical cyclone activity in the Pacific region. Regardless of the uncertainties associated with quantifying future changes in meteorological hazards, sea level is certain to rise in response to melting of continental ice caps and thermal expansion of seawater. Higher sea levels will exacerbate coastal flooding from storm surge, intense precipitation events, and tsunami inundation.

Climate change and sea-level rise are not the only future threats for coastal regions. Many coastal regions suffer from severe subsidence. In some locations the increase in subsidence is much larger than the sea-level rise. For example, in Jakarta the subsidence is currently over 10cm per year. According to Brinkman and Hartman (2008), Jakarta is heading toward a disaster with the juxtaposition of the high sea tides and the subsidence rate. Up to 4 million people and approximately 25 percent of the city will be affected by inundation from the sea within the next 15 years if action is not taken.

## Exposure <sup>17</sup>
Exposure modelling has a critical role to play in risk assessment. Empirical studies suggest that the greatest influence on output loss estimates from risk models derives from exposure data, as opposed to either hazard or vulnerability data (see for example Bal et al. 2010; Chen et al. 2004; Spence et al. 2003; Lavakare and Mawk 2008). 

The process of exposure modelling identifies the elements at risk in areas that could potentially be affected by natural hazard events (UNISDR 2009; Ehrlich and Tenerelli 2013; van Westen 2012). In other words, if a hazard occurs in an area with no exposure, there is no risk. This is the case, for example, with an earthquake in an unpopulated area of Alaska. 

Exposure modelling techniques have been developed at various scales, from global to local. Significantly, global-scale and local-scale modelling use different methodologies: the former tends to take a top-down approach, with work being carried out by governments or large institutions, whereas the latter works from the bottom up by methods such as crowdsourcing and in situ surveys. At least four homogeneous inventory regions—urban residential, urban nonresidential, rural residential, and rural nonresidential—are usually defined to capture the differences in occupancy and construction. Data sources also vary by resolution.

At the **local scale**, high-resolution exposure data have been developed on an ad hoc basis, in areas where risk modelling has been carried out. Crowdsourcing has become a common and valuable tool for collecting detailed bottom-up data, but this approach has limits, both in the type of data it can collect and in the quality of those data. In addition to being used to develop exposure data at a local scale, crowdsourcing has also been used to validate global-scale data. At the **national scale**, complete geospatially linked inventories that include public infrastructures are rare and not publicly available in most developing countries, where exposure model development is most needed for risk assessments. At the **global scale**, efforts to generate globally consistent exposure data sets in terms of quality and resolution have increased. Experience has shown that development of exposure data sets requires innovative, efficient methodologies for describing, collecting, validating, and communicating data, while also accounting for the inherent spatiotemporal dynamics associated with exposure—that is, the dynamics by which exposure evolves over time as a result of (unplanned) urbanization, demographic changes, modifications in building practices, and other factors. 

The information used to develop exposure data sets can be derived from various sources and methods. At a local level, common data sources are council and local government agencies, household surveys, aerial photos, and individual architectural/structural drawings. At a regional level and above, state-based agencies, statistical offices, census data, investment and business listings, employment figures, and existing geographic information system (GIS) data are common sources of exposure information. At the coarsest level of resolution, national statistical agencies, census data, global databases, and remote sensing are used for developing exposure data. 

Commercial risk models have developed the so-called industry exposure databases for regions where risk models are offered. These exposure data can include detailed information on construction as well as estimates of the value of the contents within a structure. The resolution of the exposure data is typically at the postal code level with varying levels of occupancy types. However, these data are almost always proprietary. 

The classification (taxonomy and ontology) used to generate these exposure data varies from data set to data set; this variation is problematic for efforts to merge independently developed data sets. Nor is there a commonly agreed upon taxonomy that accounts for features such as construction attributes and asset valuation across different hazards. 

In recent years, several data sets with global coverage have made the first step in overcoming these obstacles. The first such global exposure data set was developed in 2010 for PAGER (Prompt Assessment of Global Earthquakes for Response), a global near-real-time earthquake loss estimation system, by the U.S. Geological Survey (Jaiswal, Wald, and Porter 2010a). In addition, three global exposure databases are slated for publication in 2014, the global risk model by UNISDR (De Bono 2013; see section 3-7), the GED4GEM by the Global Earthquake Model (Dell’Acqua, Gamba, and Jaiswal 2012; see section 3-6), and the World Bank exposure database, which will be completely open and suitable for multi-hazard analyses (Gunasekera et al. 2014).<sup>18</sup> Many of these newer exposure models take advantage of aspects of building typology taxonomy originally compiled in the PAGER database. Several examples of global exposure data sets are given in box 2-5.

///Categories of information included in exposure models./// There are several categories of assets that need to be included in a comprehensive exposure model (table 2-2). The broad variety of categories illustrates the necessity of combining efforts from different disciplines, such as geographical science, statistics, engineering, mathematics, economics, remote sensing, and socio-demographics.

It is clear that as more data are integrated, modelled, and jointly analyzed, __uncertainties__ propagate in the model and in the subsequent results. A choice needs to be made about whether slightly more-detailed data will improve a model or merely add to the noise and confusion. The impossibility of eliminating uncertainty in hazard and vulnerability modelling is widely recognized. After all, every model constitutes a simplified approximation of reality. Depending on geospatial data characteristics (including resolution aspects) and integration factors, uncertainty may increase. It is therefore essential for uncertainties to be conceptually integrated into the framework of the risk analysis, and consequently into the loss estimates. The uncertainties and associated limitations in the final risk assessment then need to be communicated to the end-users of this information.

///Information required for the modelling of physical damage./// On a national scale, reliable data on physical exposure are less available than population data. Information is often missing or incomplete, and few governments have developed national exposure databases of buildings and infrastructure that are open and can be used to understand the impacts of multiple hazards (Turkey, Australia, the United States, and New Zealand are exceptions). Thus it is not surprising that most exposure data sets at the national scale or above use the spatial distribution of population as a proxy for developing exposure estimates. This is a rapidly evolving area, however, and more governments are seeing the widespread value of developing exposure information.

The basic information needed to model the response of a structure to a hazard event includes its location, occupancy, construction type, length or density (for road and railway), and replacement value. The response of a structure to a hazard event can be more realistically simulated using additional structural information such as its square footage, shape, height, age, roof type, irregularities, and material and mechanical properties, as well as building codes applicable to it. For hydrological hazards, additional details useful for vulnerability assessments include information on the height above ground of the first occupied floor, distance from water channels, and the presence of basements. Knowledge of the replacement value makes it possible to estimate the direct loss associated with an event. 

///Modelling economic losses./// Valuation data are critical for quantitatively assessing economic loss from disasters. The reinsurance industry uses claims and other economic data sets to calibrate its exposure models. However, this information is often proprietary and limited to insured risks. Obtaining comprehensive loss data for uninsured property is much more difficult. Proxy data such as socioeconomic surveys, labor statistics by economic sector, floor area per employee by type of activity, etc. are used to determine nonresidential building stock values. Accounting for a structure’s contents becomes particularly significant when modelling nonresidential occupancy classes. 

///Incorporating the temporal variation in human exposure./// Other important factors related to exposure data are population and demography characteristics that highlight the movement of population through the course of a day. Consider, for example, the swelling of populations in major metropolitan areas during the work day, or the varying population characteristics of areas of cultural or religious value depending on the day and/or time of the year. Temporal variability in human exposure can be a key factor in determining the impact of rapid onset events such as earthquakes, landslides, or tsunamis. Models of building occupancy that consider daily patterns have been proposed (Coburn and Spence 2002; Coburn, Spence, and Pomonis 1992), but collecting the necessary data to update such models can be very time- and resource-intensive. A promising alternative approach takes advantage of cellular phone data provided by telephone companies (Wesolowski et al. 2013; Lu et al. 2013).

///Exposure data collection approaches—full enumeration, sampling, or disaggregation using proxy data./// In general terms, top-down and bottom-up approaches are used to collect exposure data. Approaches that use bottom-up methods commonly employ direct observation, which relies on two principle strategies: full enumeration or sampling. With the __full enumeration__ approach, each exposed asset in the study area is detected and defined. This approach can be very accurate and detailed but also requires a greater expenditure of time and other resources. Census data are commonly used to fully enumerate human populations, though this approach is best suited to developed countries, which are likely to have slow or moderate population growth and up-to-date census data. __Volunteered geographic information__ (VGI), another approach to full enumeration, derives data from the joint efforts of many individuals who voluntarily collect and submit data. VGI may be either structured or unstructured—the latter applies to unsystematic, non-authoritative initiatives such as OpenStreetMap (see box 1-2), which rely on participants’ interest and motivation. The structured approach also involves volunteers but has an authoritative component that directs volunteers’ efforts toward certain tasks (Chapman 2012), such as a government-led participatory mapping program to collect exposure data for risk assessment (see section 3-3). 

With a __sampling__ approach, summary statistics for a large area are estimated based on smaller subset areas. Increasingly, census methodologies are turning to sampling and statistical modelling rather than full enumeration because they provide more up-to-date and more accurate information with less effort than traditional methods. A rolling census approach—in which only small areas are fully enumerated and other, highly populous areas are continuously sampled at the rate of around 10 percent a year—makes it possible to update data annually instead of every 5 to 10 years (UN Statistical Division 2008). 

__Remote sensing__ is on occasion used in conjunction with these sampling methodologies (Adams and Huyck 2006; Müller, Reiter, and Weiland 2011; Geiß and Taubenböck 2013). For instance, urban areas can be classified according to their density using satellite images, followed by a sampling approach where high-resolution imagery (manual or automatic extraction of features) or direct observation is used to fully enumerate assets (buildings, roads, bridges) and their geometric characterization (footprint, shape, height) within each of the sampling areas that represent the common density pattern classified during the first step. Alternatively, if time and resources permit, optical satellite or aerial images can be used to extract all of the footprints for buildings in an exhaustive manner. To provide a complete description of the exposure, however, the footprints should be combined with in situ direct observations or other data sets (such as national statistics information) that provide additional data that cannot be captured from above (e.g., construction features or building use).

In recent years, digital __in situ data capturing systems__ have started to emerge, which allow the user to collect and generate exposure information using handheld direct observation tools in combination with other disaggregation or extrapolation methodologies (FEMA 2002). An example includes the open source suite of tools developed under GEM called Inventory Data Capture Tools (IDCT). IDCT takes information generated from the analysis of satellite images to characterize built-up areas and combines it with sampled direct field observations on individual buildings using handheld devices or paper survey forms. This information is then integrated through the use of mapping schemes to generate exposure information. 

Indirect, top-down disaggregation approaches use exposure proxies to develop exposure data sets when direct observation alone is not feasible. Information on the spatial distribution of population and built-up areas allows the exposure to be disaggregated into finer resolutions. Some examples of this approach are described in box 2-6.

///Multi-source integration./// The growing variety of possible exposure information sources requires the flexible integration of existing information from different acquisition techniques, scales, and accuracies, so that no available information is discarded. An example for a probabilistic integration approach is given in Pittore and Wieland (2013). This method is based on Bayesian networks and allows for the sound treatment of uncertainties and for the seamless merging of different data sources, including legacy data, expert judgment, and inferences based on data mining.

There are clearly many approaches to collecting exposure data; however, for best results the decision on the approach must be aligned with the scale and purpose of the risk assessment (see box 2-7).

## Vulnerability and Loss
Vulnerability is typically described in terms of damage and/or loss. Damage and loss to a structure are assessed using functions that relate hazard intensity to damage; see figure 2-2 for an illustration. Various adjectives are used to describe the functions, including “fragility,” “damage,” and “vulnerability.” Engineers use fragility functions to quantify damage and vulnerability functions to quantify loss caused by a hazard. However, it is not uncommon to use the term vulnerability function when discussing damage. Damage is often quantified using a damage ratio where 0 is equivalent to no damage and 1 is equivalent to complete destruction. Multiplying value by the damage ratio gives an estimate of direct loss. 

The resolution of loss estimates will vary by model. For a global- or regional-scale model, the losses may resolve only total direct loss, whereas detailed site-specific models may estimate loss to a structure, its contents, and outlying buildings and include time-dependent losses such as business interruption. Site-specific fragility and vulnerability functions can account for differences in structural characteristics, such as roof covering and how it is attached. Loss estimates for contents, business interruption, and outlying structures tend to be just a simple function of loss to the main structure. Fatality estimates tend to be based on knowledge of local population and empirical relationships based on structural damage or hazard characteristics. For example, PAGER estimates fatality rates based on ground-shaking intensity and a region-specific fatality rate (Jaiswal and Wald 2010). A somewhat similar approach is used for floods, where the fatality rate is a function of flood depth (Boyd et al. 2010).

Generally, functions are defined using mean values and a coefficient of variation (CV) for a range of hazard intensities (three-second gust wind speed at 5km/hr intervals, peak ground acceleration at intervals of 0.1g, flood depth at 50cm intervals, etc.) The CV tends to decrease with more information. For example, a relatively precise (small CV) estimate of damage would be expected if one had a vulnerability function that accounted for the structural details of a building designed and built to withstand the expected hazard intensities. The damage estimate would have considerable uncertainty (large CV) if the structure were part of aggregate occupancy data. An alternative to a function that provides a mean and a CV is to use a damage probability matrix.

Methods of assessing damage vary greatly depending upon the type of exposure under consideration (e.g., people, buildings, livestock), the resolution of the exposure information (e.g., site specific or aggregate data at postal code resolution or lower), and the details available for a given resolution (e.g., whether just occupancy is known or detailed structural information is available). In addition, the choice of whether to use a mean value or a sampled value for damage depends on the details of a risk analysis. A sampled value is generated using the mean and CV from the vulnerability function at the requisite hazard intensity. Other factors that can be incorporated into damage and loss estimates include when the structure was built, given that building practices and codes have changed over time, and the timing of an event, given that the use of a structure varies over the course of a day. 
Often losses are adjusted for a variety of additional factors, such as having to replace a structure if damage exceeds a certain threshold; accounting for business interruption costs for commercial or industrial properties or additional living expenses for residential properties; incorporating the effects of demand surge on large or sequential disasters; and including damage to a structure’s contents. A good overview of loss calculations is provided in section 3-18.

Losses can be estimated ex ante and ex post. Modelled losses often differ from observed losses for a variety of reasons. One reason is that modelled losses represent only losses that are captured by the model, and these losses depend upon the quality (in terms of resolution and detail) of the exposure data. Another reason is that loss inventories are typically collected in an ad hoc manner. Better records of disaster losses would provide a range of benefits (see box 2-8).

The historical record of loss mainly represents direct tangible losses produced by an event. Examples of direct tangible loss include damage to public and private infrastructure, commercial and industrial facilities, dwellings, and the contents of a structure. The cost of business interruption and the expense of housing a structure’s inhabitants while a dwelling is repaired or replaced are considered indirect losses. Indirect losses generally arise from disruptions in the flow of goods and services, though such disruptions can produce positive as well as negative impacts. An example of a positive impact would be the increased demand for construction material. In contrast to tangible losses that are relatively easy to value, such as damage to structures or contents, intangible losses are associated with assets that are difficult to value. Examples include the loss of a life, damage to ecosystem services, and damage to sites related to cultural heritage. (Box 2-9 describes efforts to increase the resilience of heritage sites in Bhutan.) A full consideration of all direct, indirect, and intangible losses would produce much higher loss estimates than the more easily quantified and commonly seen records of direct loss.

It can be difficult to anticipate and quantify the potential for indirect losses despite their size. The 2011 Tohoku earthquake and tsunami in Japan and flooding in Thailand offer an example of the global indirect impacts from local events. The Japanese tsunami was much more spectacular and had dramatic news coverage; however, the Thailand floods caused much more damage to industrial supply chains on a global basis.

The 2011 Tohoku earthquake and tsunami slowed the Japanese and global economies. For the full year of 2011 the gross domestic product (GDP) of Japan was 0.7 percent lower than in 2010 (Trésor-Economics 2012). The largest quarterly decline (1.8 percent) occurred in the first quarter when the earthquake and tsunami struck. There was a rebound in the third quarter followed by a decline in the fourth quarter that was associated with the Thailand floods. On a global basis there was negligible impact on full-year GDP because of a rebound in the second half of 2011. In addition, spending on public sector reconstruction resulted in a positive impact in 2012. 

In contrast to the Japanese disaster, the 2011 flooding in Thailand was estimated to have reduced global production by 2.5 percent (UNISDR 2012) and reduced Thailand’s GDP growth rate from 4.0 percent to an expected 2.9 percent (World Bank 2012b). The reason Thailand’s flooding had such a dramatic impact on the global economy is that industrial parks outside of Bangkok were a critical node in the global supply chain for the production of automobiles and electronics (Haraguchi and Lall 2013). 

As box 2-8 suggests, collecting and analyzing damage and loss data from previous disasters provides valuable insight into the understanding of physical, social, and economic vulnerability. Collecting information post-disaster can build damage scenarios to inform planning processes, assess the physical and financial impact of disasters, develop preparedness measures, and facilitate dialogue for risk management. A number of global and national disaster loss systems, some open and some proprietary, record the losses associated with disasters; these are listed in table 2-3. For more detailed information, see the United Nations Development Programme survey of loss databases (UNDP 2013). 

## Hazard and Risk Assessment Tools<sup>19</sup>

Since 2005, the number of nonproprietary hazard and risk modelling tools has grown rapidly as part of the global movement to understand and manage risk. These tools allow users to calculate risk and better understand, prepare for, and mitigate the likely impact of potential disasters.

Given the plethora of tools available and the variety of reasons for seeking to assess risk, users may find it challenging to choose the appropriate tool for addressing the hazard, exposure, and/or risk question under consideration and is aligned with their modelling and computational experience. Some attempts have been made to evaluate the many modelling tools that are available to users at no cost, but these efforts did not include in-depth review or testing. Thus the evidence base to differentiate tools for different purposes and end uses has been lacking.

To address this gap and meet the need for a systematic review of tools against a set of established criteria, the Global Facility for Disaster Reduction and Recovery (GFDRR) and World Bank undertook testing and evaluation of free hazard and risk modelling software using a consistent approach. The review considered over 80 open access noncommerical software packages. A preliminary analysis based on whether the 80 models were currently supported was used to select a subset of eight earthquake models, four cyclone models, eleven flood models, and eight storm surge/tsunami models for more detailed analysis. The detailed analysis evaluated the models on the basis of over 100 criteria, and the results provide a synopsis of key open access natural hazard risk modelling tools available worldwide.  

This analysis highlights the strengths of different modelling tools, from sophisticated graphical user interfaces (GUIs) to ease of installation, to user support and frequent updates, to capacity for customization. It also highlights some of the challenges that a user of a modelling tool might face, from difficulty with installation to poor documentation and many other factors. It is important to note that many modelling tools are frequently updated, so the challenges presented in this analysis may have been overcome with a recent software update.
  
The evaluation of software packages included the following steps:

1.	.Evaluation criteria were developed for open access software packages based on Daniell (2009) and through consultations.
2.	.A preliminary review of available open source packages worldwide in the four peril types was undertaken. More than 80 software packages were downloaded and initial checks made concerning availability, source code, active or inactive status, and so on.
3.	.An initial multi-criteria analysis was undertaken in order to select the packages to review in depth for each peril. 
4.	.The 31 selected packages were installed and tested using tutorials, data sets, and examples in order to create outputs. This step included noting advantages and disadvantages of these software packages, and then filling out a detailed final set of about 180 criteria under 11 key classification themes (open source, GUI, software documentation, technology, exposure component, vulnerability, hazard, risk, post-event analysis, scenario planning, and output).

A sample page of the review (for MAEvis/mHARP) is shown in figure 2-3. The review of every package is available in Daniell (2014).

The information generated with this assessment can aid users in selecting suitable software packages. It is highly recommended that users test as many packages as possible in order to make an informed decision about which software is right for their purposes. Users at all levels should understand the sensitivity of models to changes in inputs and would probably benefit from training, as box 2-10 suggests. (For case studies that demonstrate the importance of training, see sections 3-9 and 3-12).

## Creating Platforms and Partnerships to Enable the Development of Risk Assessments

The move to collect, analyze, and produce risk information for current and future climates is gaining momentum among various actors at various levels, from the individual to the global. One consequence of this trend is a growing need for all actors involved with risk to cooperate, communicate, and form partnerships across geographic, institutional, and disciplinary boundaries. Fortunately, much progress has been made in this regard.

The recognition that cooperation and partnership are crucial for building resiliency motivated the formation in 2010 of the Understanding Risk community, whose more than 3,000 members span the globe and include experts and practitioners across many professions and disciplines (see box 2-11 for more detail). Information sharing is critical to this community, which meets every two years to discuss best practices and promising innovations in disaster risk assessment and to give members an opportunity to build and strengthen partnerships and spur further innovations. 

The Global Earthquake Model suggests some of the benefits that arise when developing and applying knowledge is treated as a cooperative endeavor.<sup>20</sup> GEM was created specifically as a public-private partnership because its founders judged that structure to be optimal for its purposes. They recognized that risk holders reside in both sectors; that advocacy, models, and information are necessary for mitigating earthquake risk; that the project could achieve its goals only by combining funds from both sectors; and that the involvement of both sectors would lend the project credibility and momentum. GEM’s formal partners include 13 private companies, 15 public organizations representing nations, and 9 international organizations. Various other associate participants and organizational members of international consortia also deliver global projects.  

One notable aspect of GEM as a public-private partnership is its success in unifying diverse perspectives under a common interest. The partnership works because both sectors seek the same outcome: credible, accessible risk information that is widely used and understood. At the same time, the two sectors have somewhat different focuses. Private sector partners generally seek to reduce future financial losses (through strict building codes and through open data that ensure common expectations of loss); to create new markets for insurance products (requiring worldwide intercomparable loss data and accessible risk information); and to build customer demand (through increased engagement among trusted local experts and increased understanding of risk by the public). Public sector partners, including nongovernmental organizations, seek to reduce future casualties, economic loss, and disruptions (through DRM and land-use policies and retrofitting of public buildings); to implement policy (requiring broad awareness of risk and hence accessible data); to base decisions on scientifically defensible hazard and risk estimates; and to reduce the need for post-disaster aid (requiring free, open information to support markets for financial risk transfer mechanisms and lower losses as a result of risk reduction). 

The perspectives and positions of the two sectors do not differ as widely as GEM’s founders initially anticipated. In practice, differences in perspective varied within each sector as much as or more than they did across sectors.  

Yet another collaboration that aims to build better risk information is the Willis Research Network, which links more than 50 international research institutions to the expertise of the financial and insurance sector in order to support scientists’ quantification of natural hazard risk. More detail on the network is in box 2-12. For an account of another kind of collaboration—one in which scientists, engineers, and developers of building codes collaborated with officials in planning, governance, and public service to promote a more earthquake-resilient city—see the account of participatory earthquake risk assessment in Dhaka in box 2-13.



